{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c13554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. src/ modules are now importable.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 01_pdf_extraction.ipynb\n",
    "# FinWise: PDF Ingestion, Cleaning, and Chunking\n",
    "# =========================\n",
    "\n",
    "# --------------------------------------\n",
    "# Step 0: Setup\n",
    "# --------------------------------------\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# If your notebook is in notebooks/, this adds the project root to sys.path\n",
    "project_root = Path().resolve().parent  # adjust if needed\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Now imports from src/ will work\n",
    "import json\n",
    "from src.data.pdf_loader import ingest_all_sources, save_docs_to_json\n",
    "from src.data.cleaners import clean_docs,split_all_cleaned_jsons\n",
    "from src.data.chunkers import chunk_docs_streaming\n",
    "\n",
    "print(\"Setup complete. src/ modules are now importable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee2d2a",
   "metadata": {},
   "source": [
    "## Step 1: Load PDFs from all sources\n",
    "\n",
    "We load PDFs from both Varsity and SEBI sources, adding rich metadata per page:\n",
    "\n",
    "- source filename\n",
    "- page number\n",
    "- category\n",
    "- total pages\n",
    "- load date\n",
    "- source path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f635cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PDFs from Varsity and SEBI sources...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading PDFs from Varsity and SEBI sources...\")\n",
    "docs = ingest_all_sources(base_raw_path=\"../data/raw\")\n",
    "print(f\"Total pages loaded: {len(docs)}\")\n",
    "\n",
    "# Compute pages per source\n",
    "pages_per_source = defaultdict(int)\n",
    "for doc in docs:\n",
    "    pages_per_source[doc.metadata['category']] += 1\n",
    "\n",
    "print(\"\\nPages loaded per source:\")\n",
    "for src, count in pages_per_source.items():\n",
    "    print(f\"- {src}: {count} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample page\n",
    "print(\"Example page metadata:\")\n",
    "print(docs[0].metadata)\n",
    "\n",
    "print(\"\\nExample page content (first 500 chars):\")\n",
    "print(docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320dee64",
   "metadata": {},
   "source": [
    "## Step 2: Save raw pages to JSON\n",
    "\n",
    "We save all pages with metadata to `data/interim/`.\n",
    "This allows us to restart cleaning or chunking without re-reading PDFs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSaving raw PDF pages to JSON files (one per source)...\")\n",
    "save_docs_to_json(docs, output_folder=\"../data/interim/raw/\")\n",
    "print(\"Raw PDF pages saved to data/interim/raw/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86bbb19",
   "metadata": {},
   "source": [
    "## Step 3: Clean the PDF pages\n",
    "\n",
    "Cleaning steps:\n",
    "\n",
    "- Remove extra whitespace\n",
    "- Remove repeated headers/footers\n",
    "- Normalize quotes and hyphens\n",
    "- Keep metadata intact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCleaning all PDF JSON files in data/interim/...\")\n",
    "clean_docs(input_folder=\"../data/interim/raw\", output_folder=\"../data/interim/cleaned\")\n",
    "print(\"Cleaning complete. Each input JSON now has a *_cleaned.json counterpart.\")\n",
    "\n",
    "# Inspect a sample cleaned page\n",
    "sample_cleaned_file = Path(\"../data/interim/cleaned\").glob(\"*_cleaned.json\")\n",
    "sample_file = next(sample_cleaned_file)\n",
    "with open(sample_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_pages = json.load(f)\n",
    "\n",
    "print(f\"Total cleaned pages in sample file: {len(cleaned_pages)}\")\n",
    "print(\"\\nExample cleaned page metadata:\")\n",
    "print(cleaned_pages[0][\"metadata\"])\n",
    "print(\"\\nExample cleaned page content (first 500 chars):\")\n",
    "print(cleaned_pages[0][\"page_content\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50dea7",
   "metadata": {},
   "source": [
    "## Step 4: Split cleaned JSONs into smaller files\n",
    "\n",
    "Splitting steps:\n",
    "\n",
    "- Automatically pick all \\*\\_cleaned.json files in data/interim/\n",
    "- Split each file into chunks of n pages (configurable)\n",
    "- Save split files with \\_partX_cleaned.json suffix for easier downstream processing\n",
    "- Keep metadata intact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSplitting all cleaned JSON files in data/interim/cleaned/..\")\n",
    "split_all_cleaned_jsons(\n",
    "    input_folder=\"../data/interim/cleaned/\",\n",
    "    output_folder=\"../data/interim/split/\",\n",
    "    pages_per_file=200\n",
    ")\n",
    "print(\"Splitting complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0946d",
   "metadata": {},
   "source": [
    "## Step 5: Chunk cleaned pages\n",
    "\n",
    "Chunking parameters:\n",
    "\n",
    "- chunk_size = 800 characters\n",
    "- overlap = 100 characters\n",
    "\n",
    "Chunks have a unique `chunk_id` and keep all page metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52633bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nChunking all cleaned JSON files in data/interim/...\")\n",
    "\n",
    "# Memory-efficient chunking\n",
    "chunk_docs_streaming(\n",
    "    input_folder=\"../data/interim/split/\",\n",
    "    output_folder=\"../data/processed/\",\n",
    "    chunk_size=500,\n",
    "    overlap=100,\n",
    "    jsonl_output=True,\n",
    ")\n",
    "\n",
    "print(\"Chunking complete. Each cleaned JSON now has a corresponding *_chunks.json file in data/processed/\")\n",
    "\n",
    "# Display number of chunks per source\n",
    "chunk_files = list(Path(\"../data/processed\").glob(\"*_chunks.json\"))\n",
    "chunks_per_source = defaultdict(int)\n",
    "\n",
    "for file in chunk_files:\n",
    "    count = 0\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        # count number of items in JSON array without loading all into memory\n",
    "        for line in f:\n",
    "            if line.strip().startswith('{'):\n",
    "                count += 1\n",
    "    source_name = file.stem.replace(\"_chunks\", \"\")\n",
    "    chunks_per_source[source_name] = count\n",
    "\n",
    "print(\"\\nChunks per source:\")\n",
    "for src, count in chunks_per_source.items():\n",
    "    print(f\"- {src}: {count} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff59af2",
   "metadata": {},
   "source": [
    "## Step 6: Summary\n",
    "\n",
    "At this point:\n",
    "\n",
    "- Raw extracted pages: `data/interim/`\n",
    "- Cleaned pages: `data/interim/`\n",
    "- Chunked pages ready for embedding: `data/processed/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "raw_folder = Path(\"../data/interim/raw/\")\n",
    "cleaned_folder = Path(\"../data/interim/cleaned/\")\n",
    "split_folder = Path(\"../data/interim/split/\")\n",
    "processed_folder = Path(\"../data/processed/\")\n",
    "\n",
    "def count_json_lines(file_path):\n",
    "    \"\"\"Count number of JSON objects in a file (works for JSONL or JSON array).\"\"\"\n",
    "    count = 0\n",
    "    try:\n",
    "        if file_path.suffix == \".jsonl\":\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for _ in f:\n",
    "                    count += 1\n",
    "        elif file_path.suffix == \".json\":\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                count = len(data)\n",
    "    except Exception:\n",
    "        count = 0\n",
    "    return count\n",
    "\n",
    "# Count files\n",
    "raw_files = list(raw_folder.glob(\"*.json\"))\n",
    "cleaned_files = list(cleaned_folder.glob(\"*_cleaned.json\"))\n",
    "split_files = list(split_folder.glob(\"*.json\"))\n",
    "chunked_files = list(processed_folder.glob(\"*_chunks.jsonl\"))\n",
    "\n",
    "# Totals\n",
    "total_raw_pages = sum(count_json_lines(f) for f in raw_files)\n",
    "total_cleaned_pages = sum(count_json_lines(f) for f in cleaned_files)\n",
    "total_split_pages = sum(count_json_lines(f) for f in split_files)\n",
    "total_chunks = sum(count_json_lines(f) for f in chunked_files)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nâœ… PDF extraction, cleaning, and chunking complete!\\n\")\n",
    "print(f\"ðŸ“‚ Raw JSON files ({raw_folder}): {len(raw_files)}\")\n",
    "print(f\"    Total raw pages: {total_raw_pages}\")\n",
    "print(f\"ðŸ“‚ Cleaned JSON files ({cleaned_folder}): {len(cleaned_files)}\")\n",
    "print(f\"    Total cleaned pages: {total_cleaned_pages}\")\n",
    "print(f\"ðŸ“‚ Split JSON files ({split_folder}): {len(split_files)}\")\n",
    "print(f\"    Total split pages: {total_split_pages}\")\n",
    "print(f\"ðŸ“‚ Chunked JSONL files ({processed_folder}): {len(chunked_files)}\")\n",
    "print(f\"    Total chunks: {total_chunks}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
