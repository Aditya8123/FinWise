{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c13554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. src/ modules are now importable.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 01_pdf_extraction.ipynb\n",
    "# FinWise: PDF Ingestion, Cleaning, and Chunking\n",
    "# =========================\n",
    "\n",
    "# --------------------------------------\n",
    "# Step 0: Setup\n",
    "# --------------------------------------\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# If your notebook is in notebooks/, this adds the project root to sys.path\n",
    "project_root = Path().resolve().parent  # adjust if needed\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Now imports from src/ will work\n",
    "import json\n",
    "from src.data.pdf_loader import ingest_all_sources, save_docs_to_json\n",
    "from src.data.cleaners import clean_docs,split_all_cleaned_jsons\n",
    "from src.data.chunkers import chunk_docs_streaming\n",
    "\n",
    "print(\"Setup complete. src/ modules are now importable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee2d2a",
   "metadata": {},
   "source": [
    "## Step 1: Load PDFs from all sources\n",
    "\n",
    "We load PDFs from both Varsity and SEBI sources, adding rich metadata per page:\n",
    "\n",
    "- source filename\n",
    "- page number\n",
    "- category\n",
    "- total pages\n",
    "- load date\n",
    "- source path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f635cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PDFs from Varsity and SEBI sources...\n",
      "Total pages loaded: 2929\n",
      "\n",
      "Pages loaded per source:\n",
      "- varsity: 2186 pages\n",
      "- sebi_education: 743 pages\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading PDFs from Varsity and SEBI sources...\")\n",
    "docs = ingest_all_sources(base_raw_path=\"../data/raw\")\n",
    "print(f\"Total pages loaded: {len(docs)}\")\n",
    "\n",
    "# Compute pages per source\n",
    "pages_per_source = defaultdict(int)\n",
    "for doc in docs:\n",
    "    pages_per_source[doc.metadata['category']] += 1\n",
    "\n",
    "print(\"\\nPages loaded per source:\")\n",
    "for src, count in pages_per_source.items():\n",
    "    print(f\"- {src}: {count} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de26a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example page metadata:\n",
      "{'source': 'varsity_module01_introduction_to_stock_markets.pdf', 'source_path': '..\\\\data\\\\raw\\\\varsity\\\\varsity_module01_introduction_to_stock_markets.pdf', 'category': 'varsity', 'page_number': 1, 'page_label': '1', 'total_pages': 111, 'text_length': 58, 'load_date': '2025-08-14T16:32:15.043596'}\n",
      "\n",
      "Example page content (first 500 chars):\n",
      "Introduction to \n",
      "Stock Markets\n",
      "ZERODHA.COM/VARSITY\n",
      "ZERODHA\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample page\n",
    "print(\"Example page metadata:\")\n",
    "print(docs[0].metadata)\n",
    "\n",
    "print(\"\\nExample page content (first 500 chars):\")\n",
    "print(docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320dee64",
   "metadata": {},
   "source": [
    "## Step 2: Save raw pages to JSON\n",
    "\n",
    "We save all pages with metadata to `data/interim/`.\n",
    "This allows us to restart cleaning or chunking without re-reading PDFs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ddd7790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving raw PDF pages to JSON files (one per source)...\n",
      "Saved 2186 pages to ..\\data\\interim\\raw\\all_pdfs_varsity.json\n",
      "Saved 743 pages to ..\\data\\interim\\raw\\all_pdfs_sebi_education.json\n",
      "Raw PDF pages saved to data/interim/raw/\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSaving raw PDF pages to JSON files (one per source)...\")\n",
    "save_docs_to_json(docs, output_folder=\"../data/interim/raw/\")\n",
    "print(\"Raw PDF pages saved to data/interim/raw/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86bbb19",
   "metadata": {},
   "source": [
    "## Step 3: Clean the PDF pages\n",
    "\n",
    "Cleaning steps:\n",
    "\n",
    "- Remove extra whitespace\n",
    "- Remove repeated headers/footers\n",
    "- Normalize quotes and hyphens\n",
    "- Keep metadata intact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d21c195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning all PDF JSON files in data/interim/...\n",
      "Saved 743 cleaned pages to ..\\data\\interim\\cleaned\\all_pdfs_sebi_education_cleaned.json\n",
      "Saved 2186 cleaned pages to ..\\data\\interim\\cleaned\\all_pdfs_varsity_cleaned.json\n",
      "Cleaning complete. Each input JSON now has a *_cleaned.json counterpart.\n",
      "Total cleaned pages in sample file: 743\n",
      "\n",
      "Example cleaned page metadata:\n",
      "{'source': 'sebi_buyback_open_offer.pdf', 'source_path': '..\\\\data\\\\raw\\\\sebi_education\\\\sebi_buyback_open_offer.pdf', 'category': 'sebi_education', 'page_number': 1, 'page_label': '1', 'total_pages': 32, 'text_length': 35, 'load_date': '2025-08-14T16:34:03.027404'}\n",
      "\n",
      "Example cleaned page content (first 500 chars):\n",
      "Buyback of and Open Offer of Shares\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCleaning all PDF JSON files in data/interim/...\")\n",
    "clean_docs(input_folder=\"../data/interim/raw\", output_folder=\"../data/interim/cleaned\")\n",
    "print(\"Cleaning complete. Each input JSON now has a *_cleaned.json counterpart.\")\n",
    "\n",
    "# Inspect a sample cleaned page\n",
    "sample_cleaned_file = Path(\"../data/interim/cleaned\").glob(\"*_cleaned.json\")\n",
    "sample_file = next(sample_cleaned_file)\n",
    "with open(sample_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_pages = json.load(f)\n",
    "\n",
    "print(f\"Total cleaned pages in sample file: {len(cleaned_pages)}\")\n",
    "print(\"\\nExample cleaned page metadata:\")\n",
    "print(cleaned_pages[0][\"metadata\"])\n",
    "print(\"\\nExample cleaned page content (first 500 chars):\")\n",
    "print(cleaned_pages[0][\"page_content\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50dea7",
   "metadata": {},
   "source": [
    "## Step 4: Split cleaned JSONs into smaller files\n",
    "\n",
    "Splitting steps:\n",
    "\n",
    "- Automatically pick all \\*\\_cleaned.json files in data/interim/\n",
    "- Split each file into chunks of n pages (configurable)\n",
    "- Save split files with \\_partX_cleaned.json suffix for easier downstream processing\n",
    "- Keep metadata intact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6284e2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting all cleaned JSON files in data/interim/cleaned/..\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_sebi_education_cleaned_part1.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_sebi_education_cleaned_part2.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_sebi_education_cleaned_part3.json\n",
      "Saved 143 pages to ..\\data\\interim\\split\\all_pdfs_sebi_education_cleaned_part4.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_varsity_cleaned_part1.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_varsity_cleaned_part2.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_varsity_cleaned_part3.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_varsity_cleaned_part4.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_varsity_cleaned_part5.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_varsity_cleaned_part6.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_varsity_cleaned_part7.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_varsity_cleaned_part8.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_varsity_cleaned_part9.json\n",
      "Saved 200 pages to ..\\data\\interim\\split\\all_pdfs_varsity_cleaned_part10.json\n",
      "Saved 186 pages to ..\\data\\interim\\split\\all_pdfs_varsity_cleaned_part11.json\n",
      "\n",
      "âœ… All *_cleaned.json files have been split.\n",
      "Splitting complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSplitting all cleaned JSON files in data/interim/cleaned/..\")\n",
    "split_all_cleaned_jsons(\n",
    "    input_folder=\"../data/interim/cleaned/\",\n",
    "    output_folder=\"../data/interim/split/\",\n",
    "    pages_per_file=200\n",
    ")\n",
    "print(\"Splitting complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0946d",
   "metadata": {},
   "source": [
    "## Step 5: Chunk cleaned pages\n",
    "\n",
    "Chunking parameters:\n",
    "\n",
    "- chunk_size = 800 characters\n",
    "- overlap = 100 characters\n",
    "\n",
    "Chunks have a unique `chunk_id` and keep all page metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52633bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunking all cleaned JSON files in data/interim/...\n",
      "\n",
      "Processing all_pdfs_sebi_education_cleaned_part1.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_sebi_education_cleaned_part1.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 86037.01page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_sebi_education_cleaned_part1_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_sebi_education_cleaned_part2.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_sebi_education_cleaned_part2.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 73103.34page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_sebi_education_cleaned_part2_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_sebi_education_cleaned_part3.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_sebi_education_cleaned_part3.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 53305.00page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_sebi_education_cleaned_part3_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_sebi_education_cleaned_part4.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_sebi_education_cleaned_part4.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 28291.77page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_sebi_education_cleaned_part4_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_varsity_cleaned_part1.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunking all_pdfs_varsity_cleaned_part1.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 16142.49page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_varsity_cleaned_part1_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_varsity_cleaned_part10.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_varsity_cleaned_part10.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 16257.31page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_varsity_cleaned_part10_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_varsity_cleaned_part11.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_varsity_cleaned_part11.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:00<00:00, 16584.62page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_varsity_cleaned_part11_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_varsity_cleaned_part2.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_varsity_cleaned_part2.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 19397.87page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_varsity_cleaned_part2_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_varsity_cleaned_part3.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_varsity_cleaned_part3.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 15938.53page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_varsity_cleaned_part3_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_varsity_cleaned_part4.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_varsity_cleaned_part4.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 43484.57page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_varsity_cleaned_part4_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_varsity_cleaned_part5.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_varsity_cleaned_part5.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 25341.70page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_varsity_cleaned_part5_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_varsity_cleaned_part6.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_varsity_cleaned_part6.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 19374.12page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_varsity_cleaned_part6_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_varsity_cleaned_part7.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_varsity_cleaned_part7.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 16686.11page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_varsity_cleaned_part7_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_varsity_cleaned_part8.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_varsity_cleaned_part8.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 43986.20page/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_varsity_cleaned_part8_chunks.jsonl\n",
      "\n",
      "Processing all_pdfs_varsity_cleaned_part9.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking all_pdfs_varsity_cleaned_part9.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 23607.94page/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunks to ..\\data\\processed\\all_pdfs_varsity_cleaned_part9_chunks.jsonl\n",
      "Chunking complete. Each cleaned JSON now has a corresponding *_chunks.json file in data/processed/\n",
      "\n",
      "Chunks per source:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nChunking all cleaned JSON files in data/interim/...\")\n",
    "\n",
    "# Memory-efficient chunking\n",
    "chunk_docs_streaming(\n",
    "    input_folder=\"../data/interim/split/\",\n",
    "    output_folder=\"../data/processed/\",\n",
    "    chunk_size=800,\n",
    "    overlap=100,\n",
    "    jsonl_output=True,\n",
    ")\n",
    "\n",
    "print(\"Chunking complete. Each cleaned JSON now has a corresponding *_chunks.json file in data/processed/\")\n",
    "\n",
    "# Display number of chunks per source\n",
    "chunk_files = list(Path(\"../data/processed\").glob(\"*_chunks.json\"))\n",
    "chunks_per_source = defaultdict(int)\n",
    "\n",
    "for file in chunk_files:\n",
    "    count = 0\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        # count number of items in JSON array without loading all into memory\n",
    "        for line in f:\n",
    "            if line.strip().startswith('{'):\n",
    "                count += 1\n",
    "    source_name = file.stem.replace(\"_chunks\", \"\")\n",
    "    chunks_per_source[source_name] = count\n",
    "\n",
    "print(\"\\nChunks per source:\")\n",
    "for src, count in chunks_per_source.items():\n",
    "    print(f\"- {src}: {count} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff59af2",
   "metadata": {},
   "source": [
    "## Step 6: Summary\n",
    "\n",
    "At this point:\n",
    "\n",
    "- Raw extracted pages: `data/interim/`\n",
    "- Cleaned pages: `data/interim/`\n",
    "- Chunked pages ready for embedding: `data/processed/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b10faad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… PDF extraction, cleaning, and chunking complete!\n",
      "\n",
      "ðŸ“‚ Raw JSON files (..\\data\\interim\\raw): 2\n",
      "    Total raw pages: 2929\n",
      "ðŸ“‚ Cleaned JSON files (..\\data\\interim\\cleaned): 2\n",
      "    Total cleaned pages: 2929\n",
      "ðŸ“‚ Split JSON files (..\\data\\interim\\split): 15\n",
      "    Total split pages: 2929\n",
      "ðŸ“‚ Chunked JSONL files (..\\data\\processed): 15\n",
      "    Total chunks: 5846\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "raw_folder = Path(\"../data/interim/raw/\")\n",
    "cleaned_folder = Path(\"../data/interim/cleaned/\")\n",
    "split_folder = Path(\"../data/interim/split/\")\n",
    "processed_folder = Path(\"../data/processed/\")\n",
    "\n",
    "def count_json_lines(file_path):\n",
    "    \"\"\"Count number of JSON objects in a file (works for JSONL or JSON array).\"\"\"\n",
    "    count = 0\n",
    "    try:\n",
    "        if file_path.suffix == \".jsonl\":\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for _ in f:\n",
    "                    count += 1\n",
    "        elif file_path.suffix == \".json\":\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                count = len(data)\n",
    "    except Exception:\n",
    "        count = 0\n",
    "    return count\n",
    "\n",
    "# Count files\n",
    "raw_files = list(raw_folder.glob(\"*.json\"))\n",
    "cleaned_files = list(cleaned_folder.glob(\"*_cleaned.json\"))\n",
    "split_files = list(split_folder.glob(\"*.json\"))\n",
    "chunked_files = list(processed_folder.glob(\"*_chunks.jsonl\"))\n",
    "\n",
    "# Totals\n",
    "total_raw_pages = sum(count_json_lines(f) for f in raw_files)\n",
    "total_cleaned_pages = sum(count_json_lines(f) for f in cleaned_files)\n",
    "total_split_pages = sum(count_json_lines(f) for f in split_files)\n",
    "total_chunks = sum(count_json_lines(f) for f in chunked_files)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nâœ… PDF extraction, cleaning, and chunking complete!\\n\")\n",
    "print(f\"ðŸ“‚ Raw JSON files ({raw_folder}): {len(raw_files)}\")\n",
    "print(f\"    Total raw pages: {total_raw_pages}\")\n",
    "print(f\"ðŸ“‚ Cleaned JSON files ({cleaned_folder}): {len(cleaned_files)}\")\n",
    "print(f\"    Total cleaned pages: {total_cleaned_pages}\")\n",
    "print(f\"ðŸ“‚ Split JSON files ({split_folder}): {len(split_files)}\")\n",
    "print(f\"    Total split pages: {total_split_pages}\")\n",
    "print(f\"ðŸ“‚ Chunked JSONL files ({processed_folder}): {len(chunked_files)}\")\n",
    "print(f\"    Total chunks: {total_chunks}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
